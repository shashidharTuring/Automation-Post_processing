import streamlit as st
import json
import pandas as pd
import numpy as np
import io
import uuid
from datetime import datetime, timezone
from jsonschema import Draft7Validator, ValidationError
from typing import Any, Dict, List, Optional
import os
from dotenv import load_dotenv
import openai

# =============================================================================
# CUSTOM CSS (responsive width + visual polish)
# =============================================================================
st.set_page_config(page_title="RLHF & JSON Viewer", layout="wide")
CUSTOM_CSS = """
<style>
:root {
  --rlhf-blue: #1E88E5;
  --rlhf-green: #43A047;
  --rlhf-purple: #8E24AA;
  --rlhf-amber: #FDD835;
  --rlhf-gray-bg: #f7f9fc;
  --rlhf-gray-border: #dfe2e8;
  --rlhf-card-radius: 8px;
  --rlhf-card-pad: 0.75rem 1rem;
  --rlhf-font-sm: 0.86rem;
  --rlhf-font-xs: 0.75rem;
}

/* widen main content */
main .block-container {
  max-width: 95vw !important;
  padding-left: 2rem !important;
  padding-right: 2rem !important;
}

/* sidebar */
[data-testid="stSidebar"] {
  background: var(--rlhf-gray-bg);
  border-right: 1px solid var(--rlhf-gray-border);
}

/* chips */
.rlhf-chip {
  display: inline-block;
  padding: 2px 8px;
  margin: 0 4px 4px 0;
  font-size: var(--rlhf-font-xs);
  border-radius: 12px;
  background: var(--rlhf-blue);
  color: white;
  white-space: nowrap;
}
.rlhf-chip-green { background: var(--rlhf-green); }
.rlhf-chip-purple { background: var(--rlhf-purple); }
.rlhf-chip-amber { background: var(--rlhf-amber); color: #333; }

/* kpi bar */
.rlhf-kpi-value {
  font-weight: 600;
  font-size: 1.05rem;
  margin-bottom: 0;
  line-height: 1.2;
}
.rlhf-kpi-label {
  font-size: var(--rlhf-font-xs);
  color: #666;
  margin-top: -2px;
}

/* role badges */
.rlhf-badge-user, .rlhf-badge-assistant {
  padding: 0 6px;
  border-radius: 6px;
  font-size: var(--rlhf-font-xs);
}
.rlhf-badge-user { background: var(--rlhf-blue); color: #fff; }
.rlhf-badge-assistant { background: var(--rlhf-green); color: #fff; }

/* code blocks */
.rlhf-code-scroll pre, code {
  max-width: 100%;
  white-space: pre-wrap;
  word-break: break-word;
}
.rlhf-code-scroll pre {
  max-height: 260px;
  overflow-y: auto;
  overflow-x: auto;
}

/* sticky toolbar */
.rlhf-toolbar-sticky {
  position: sticky;
  top: 48px;
  z-index: 49;
  padding: 0.5rem;
  background: #ffffffeb;
  backdrop-filter: blur(4px);
  border-bottom: 1px solid var(--rlhf-gray-border);
  margin-bottom: 0.5rem;
}

/* delivery summary */
.rlhf-delivery-summary {
  font-size: var(--rlhf-font-sm);
  margin-bottom: 0.5rem;
}
.rlhf-delivery-summary strong { color: var(--rlhf-blue); }

/* validator dropzone */
.rlhf-dropzone {
  padding: 2rem;
  border: 2px dashed var(--rlhf-gray-border);
  border-radius: var(--rlhf-card-radius);
  background: var(--rlhf-gray-bg);
  text-align: center;
}
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

# =============================================================================
# MAIN PAGE TABS
# =============================================================================
st.title("ğŸ” Choose Viewer Mode")
option = st.radio(
    label="Select a Viewer",
    # â¬…ï¸  add â€œSchema Validatorâ€ here
    options=["Home", "RLHF Viewer", "JSON Visualizer", "Schema Validator"],
    index=0,
    horizontal=True
)




# =============================================================================
# BASIC UTILITIES
# =============================================================================
def load_json(uploaded_file):
    if uploaded_file is None:
        return None
    try:
        return json.load(uploaded_file)
    except Exception as e:  # noqa: BLE001
        st.error(f"Failed to parse JSON: {e}")
        return None


def current_utc_iso_millis() -> str:
    now = datetime.now(timezone.utc)
    return now.strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "Z"


def timestamped_json_filename(prefix="rlhf_final_payload", suffix=".json") -> str:
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    return f"{prefix}_{ts}{suffix}"


def build_annotator_task_df(data_sft: List[Dict[str, Any]]) -> pd.DataFrame:
    annotator_task_dict = {}
    for task_ in data_sft:
        task_id = task_.get("id")
        annotator_id = task_.get("humanRoleId")
        annotator_task_dict.setdefault(annotator_id, []).append(task_id)

    annotator_task_df = (
        pd.Series(annotator_task_dict)
        .explode()
        .reset_index(name="task_id")
        .rename(columns={"index": "AnnotatorID"})
    )
    annotator_task_df["task_id_orig"] = annotator_task_df["task_id"]
    annotator_task_df["task_id"] = pd.to_numeric(
        annotator_task_df["task_id"], errors="coerce"
    )
    return annotator_task_df


def compile_task_rows(
    task_id_map: Dict[str, Dict[str, Any]],
    data_sft: List[Dict[str, Any]],
    annotator_task_df: pd.DataFrame,
) -> pd.DataFrame:
    compiled_rows: List[Dict[str, Any]] = []
    
    for task_id, task in task_id_map.items():
        # Base row information that's common to all turns
        base_row: Dict[str, Any] = {
            "task_id": task_id,
            "colab_link": task.get("task", {}).get("colabLink", ""),
        }
        
        # Handle scope requirements
        scope = task.get("metadata", {}).get("scope_requirements", {})
        for k, v in scope.items():
            base_row[f"scope_{k}"] = v
        
        messages = task.get("messages", [])
        
        # Extract system message (should be first and only one)
        system_msg = next(
            (m for m in messages if m.get("role") == "system"), None
        )
        if system_msg:
            base_row["system_message"] = system_msg.get("text", "")
        
        # Get user and assistant messages in order
        user_messages = [m for m in messages if m.get("role") == "user"]
        assistant_messages = [m for m in messages if m.get("role") == "assistant"]
        
        # Create a row for each user-assistant pair
        max_turns = max(len(user_messages), len(assistant_messages))
        
        for turn_idx in range(max_turns):
            row = base_row.copy()
            row["turn_number"] = turn_idx + 1
            
            # Handle user message for this turn
            if turn_idx < len(user_messages):
                user_msg = user_messages[turn_idx]
                
                # print(f"User Message (Turn {turn_idx + 1}):")
                # print("")
                # print(user_msg)
                # print("_______" * 9)
                
                row["prompt"] = user_msg.get("text", "")
                
                # Handle prompt evaluation
                for entry in user_msg.get("prompt_evaluation", []):
                    q = entry.get("question", "").strip().replace(" ", "_").lower()
                    row[f"prompt_eval_{q}"] = entry.get("human_input_value", "")
            else:
                row["prompt"] = ""
            
            # Handle assistant message for this turn
            if turn_idx < len(assistant_messages):
                assistant_msg = assistant_messages[turn_idx]
                signal = assistant_msg.get("signal", {})
                
                row["ideal_response"] = signal.get("ideal_response", "")
                
                # Handle assistant evaluation
                for eval_set in signal.get("human_evals", []):
                    for item in eval_set.get("evaluation_form", []):
                        q = item.get("question", "").strip().replace(" ", "_").lower()
                        row[f"assistant_eval_{q}"] = item.get("human_input_value", "")
            else:
                row["ideal_response"] = ""
            
            # Add metadata about the conversation
            row["total_user_messages"] = len(user_messages)
            row["total_assistant_messages"] = len(assistant_messages)
            row["total_turns"] = max_turns
            
            compiled_rows.append(row)
    
    if not compiled_rows:
        return pd.DataFrame()
    
    df = pd.DataFrame(compiled_rows)
    df["task_id_orig"] = df["task_id"]
    df["task_id"] = pd.to_numeric(df["task_id"], errors="coerce")
    df = df.merge(annotator_task_df, on="task_id", how="left")
    
    return df


def _safe_show_eval_rows(rows: List[Dict[str, Any]]):
    df = pd.DataFrame(rows)
    if not df.empty:
        df = df.applymap(lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x)
        df = df.astype(str)
    st.table(df)


# =============================================================================
# FILTERS (multi-select, targeted cols)
# =============================================================================
ALLOWED_FILTER_COLS = ["task_id", "scope_category", "scope_batchName", "scope_batchId"]


def filter_dataframe(
    df: pd.DataFrame, allowed_cols: Optional[List[str]] = None
) -> pd.DataFrame:
    if allowed_cols is None:
        allowed_cols = ALLOWED_FILTER_COLS

    out = df.copy()
    cols_present = [c for c in allowed_cols if c in df.columns]
    cols_missing = [c for c in allowed_cols if c not in df.columns]
    if cols_missing:
        st.caption("Missing filter columns: " + ", ".join(cols_missing))
    if not cols_present:
        return out

    # 2-column grid for widgets
    n_cols = 2
    col_widgets = st.columns(n_cols)
    sel_summary = []

    for i, col in enumerate(cols_present):
        idx = i % n_cols
        container = col_widgets[idx]
        with container:
            series = df[col]
            non_null = series.dropna()

            uniq_vals = non_null.unique().tolist()
            disp_map = {str(v): v for v in uniq_vals}
            disp_labels = sorted(disp_map.keys(), key=lambda x: (len(x), x))

            sel_labels = st.multiselect(
                col,
                options=disp_labels,
                default=[],
                key=f"flt_ms_{col}",
                help=f"Filter rows where {col} matches selected values.",
            )
            if sel_labels:
                sel_vals = [disp_map[l] for l in sel_labels]
                out = out[out[col].isin(sel_vals)]
                sel_summary.append((col, sel_labels))

    if sel_summary:
        chip_html = ""
        for col, labels in sel_summary:
            for val in labels:
                chip_html += f"<span class='rlhf-chip-amber'>{col}: {val}</span>"
        st.markdown("**Active filters:** " + chip_html, unsafe_allow_html=True)

    return out


# =============================================================================
# DELIVERY HELPERS
# =============================================================================
def row_to_record(row: pd.Series, eval_cols: List[str]) -> Dict[str, Any]:
    print("Inside row_to_record")
    meta = {col: row[col] for col in eval_cols if col in row.index}
    meta = {k: (None if pd.isna(v) else v) for k, v in meta.items()}
    
    question = (
        row["prompt"] if "prompt" in row and not pd.isna(row["prompt"]) else None
    )
    print(f"Question: {question}")
    answer = (
        row["ideal_response"]
        if "ideal_response" in row and not pd.isna(row["ideal_response"])
        else None
    )
    if "system_message" in eval_cols:
        system_message = (
        row["system_message"]
        if "system_message" in row and not pd.isna(row["system_message"])
        else None
    )
        return {"system_message": system_message, "question": question, "answer": answer, "metadata": meta}
    else:
        return {"question": question, "answer": answer, "metadata": meta}




# =============================================================================
# MODIFIED build_delivery_json_from_ingestion FUNCTION
# =============================================================================
def build_delivery_json_from_ingestion(
    ingestion_data: Dict[str, Any],
    data_csv: pd.DataFrame,
    data_sft: List[Dict[str, Any]],
    pdf_col: str = "scope_pdf_name",
    min_turns: int = 3,  # New parameter for minimum turns
) -> Dict[str, Any]:
    if ingestion_data is None:
        return {}

    sft_annotation = []
    for task in data_sft:
        subset = {k: task.get(k) for k in ("createdAt", "id", "humanRoleId")}
        sft_annotation.append(subset)
    sft_annotation_df = pd.DataFrame(sft_annotation)

    if pdf_col not in data_csv.columns:
        st.warning(
            f"Column `{pdf_col}` not found; using single dummy group for delivery batch build."
        )
        data_csv = data_csv.copy()
        data_csv[pdf_col] = "_NO_PDF_KEY_"

    work_item_ids = [w.get("workItemId") for w in ingestion_data.get("workitems", [])]
    if not work_item_ids:
        st.error("Ingestion JSON has no `workitems` with `workItemId`.")
        return {}

    pdf_unique = data_csv[pdf_col].dropna().unique().tolist()
    deliveries_gen = min(len(pdf_unique), len(work_item_ids))
    work_item_ids_needed = work_item_ids[:deliveries_gen]

    eval_cols = [col for col in data_csv.columns if "eval" in col.lower()]

    master_list = []
    skipped_groups = []
    Counter_1 = 0
    counter_2 = 0
    
    for index, work_id in enumerate(work_item_ids_needed):
        if index >= len(pdf_unique):
            break
        pdf_name = pdf_unique[index]
        data_tmp = data_csv.loc[data_csv[pdf_col] == pdf_name].reset_index(drop=True)
        Counter_1 += 1
        
        # Use user-defined minimum turns instead of hardcoded 3
        if data_tmp.shape[0] < min_turns:
            counter_2 += 1
            skipped_groups.append((pdf_name, data_tmp.shape[0]))
            continue

        annot_vals = (
            data_tmp["AnnotatorID"].dropna().unique().tolist()
            if "AnnotatorID" in data_tmp
            else []
        )
        annotator_id = str(annot_vals[0]) if annot_vals else "NA"

        records = [row_to_record(r, eval_cols) for _, r in data_tmp.iterrows()]
        task_answers = {"id": pdf_name, "annotations": records}
        task_answers_dict = {"taskAnswers": [task_answers]}

        variation_task_ids = data_tmp["task_id"].unique().tolist()
        try:
            labelled_ts = (
                sft_annotation_df[sft_annotation_df["id"].isin(variation_task_ids)][
                    "createdAt"
                ]
                .tolist()[-1]
            )
        except Exception:  # noqa: BLE001
            labelled_ts = current_utc_iso_millis()

        metadata_schema = {
            "taskId": str(uuid.uuid4()),
            "operationType": "LABELLING",
            "labelledTimestamp": labelled_ts,
            "obfuscatedDaAlias": annotator_id,
        }

        uid_data = {"data": task_answers_dict, "metadata": metadata_schema}
        tmp_dict = {
            "workItemId": work_id,
            "workflow": "workflow_name",
            "locale": "en_US",
            "inputData": {"Document": pdf_name},
            "metadata": {},
            str(uuid.uuid4()): [uid_data],
        }
        master_list.append(tmp_dict)

    # Show skipped groups info with user-defined minimum
    if skipped_groups:
        st.warning(
            f"Skipped {len(skipped_groups)} groups with fewer than {min_turns} turns: "
            + ", ".join([f"{name}({n})" for name, n in skipped_groups[:5]])  # Show first 5
            + ("..." if len(skipped_groups) > 5 else "")
        )

    return {
        "fileMetadata": ingestion_data.get("fileMetadata"),
        "workitems": master_list,
    }




# =============================================================================
# TABS
# =============================================================================
if option == "RLHF Viewer":
    st.title("ğŸ§  RLHF Task Viewer")

    # =============================================================================
    # SIDEBAR â€“ DELIVERY RLHF JSON
    # =============================================================================
    st.sidebar.header("ğŸ“ Upload RLHF Delivery JSON")
    json_file = st.sidebar.file_uploader(
        "Upload delivery JSON", type="json", key="delivery_json_uploader"
    )

    if not json_file:
        st.info("ğŸ‘ˆ Upload a Delivery RLHF JSON to begin.")
        st.stop()

    data = load_json(json_file)
    if data is None:
        st.stop()

    if "rlhf" not in data:
        st.error("âŒ Uploaded JSON missing top-level `rlhf` key.")
        st.stop()
    if "sft" not in data:
        st.error("âŒ Uploaded JSON missing top-level `sft` key.")
        st.stop()

    rlhf_tasks = data["rlhf"]
    data_sft = data["sft"]

    annotator_task_df = build_annotator_task_df(data_sft)
    task_id_map = {str(task["taskId"]): task for task in rlhf_tasks if "taskId" in task}
    if not task_id_map:
        st.warning("âš ï¸ No valid RLHF tasks with `taskId` found.")
        st.stop()

    data_csv = compile_task_rows(task_id_map, data_sft, annotator_task_df)


    # =============================================================================
    # KPI STRIP
    # =============================================================================
    total_tasks = len(task_id_map)
    unique_annotators = annotator_task_df["AnnotatorID"].nunique()
    scope_cat = data_csv["scope_category"] if "scope_category" in data_csv.columns else None
    unique_categories = int(scope_cat.nunique()) if scope_cat is not None else 0
    scope_batch = data_csv["scope_batchId"] if "scope_batchId" in data_csv.columns else None
    unique_batches = int(scope_batch.nunique()) if scope_batch is not None else 0

    k1, k2, k3, k4 = st.columns(4)
    with k1:
        st.markdown(
            f"<div class='rlhf-kpi-value'>{total_tasks}</div>"
            "<div class='rlhf-kpi-label'>Tasks</div>",
            unsafe_allow_html=True,
        )
    with k2:
        st.markdown(
            f"<div class='rlhf-kpi-value'>{unique_annotators}</div>"
            "<div class='rlhf-kpi-label'>Annotators</div>",
            unsafe_allow_html=True,
        )
    with k3:
        st.markdown(
            f"<div class='rlhf-kpi-value'>{unique_categories}</div>"
            "<div class='rlhf-kpi-label'>Categories</div>",
            unsafe_allow_html=True,
        )
    with k4:
        st.markdown(
            f"<div class='rlhf-kpi-value'>{unique_batches}</div>"
            "<div class='rlhf-kpi-label'>Batches</div>",
            unsafe_allow_html=True,
        )

    inspect_tab, csv_tab, delivery_tab, validator_tab = st.tabs(
        [
            "ğŸ” Inspect Task ID",
            "ğŸ§¾ View CSV",
            "ğŸ“¦ Delivery Batch Creator"
        ]
    )
    


    # =============================================================================
    # TAB 1 â€“ INSPECT TASK
    # =============================================================================
    with inspect_tab:
        st.subheader("Inspect a Single Task")

        selected_task_id = st.selectbox(
            "Select a Task ID", list(task_id_map.keys()), key="inspect_task_id_select"
        )

        if selected_task_id:
            task = task_id_map[selected_task_id]

            # wider message column
            meta_col, msg_col = st.columns([0.9, 2.1])

            # --- metadata / scope -----------------------------------------------------
            with meta_col:
                st.markdown(f"### ğŸ“„ Task `{selected_task_id}`")
                colab_link = task.get("task", {}).get("colabLink", "N/A")
                st.markdown(f"ğŸ”— **RLHF Link**: [Open Task]({colab_link})")

                st.markdown("#### ğŸ“˜ Scope")
                scope = task.get("metadata", {}).get("scope_requirements", {})
                scope_rows = [{"Key": k, "Value": str(v)} for k, v in scope.items()]

                # annotator
                try:
                    annotator_id_list = annotator_task_df[
                        annotator_task_df["task_id"] == int(selected_task_id)
                    ]["AnnotatorID"].tolist()
                    annotator_id = annotator_id_list[0] if annotator_id_list else "NA"
                except Exception:  # noqa: BLE001
                    annotator_id = "NA"
                scope_rows.append({"Key": "annotator_id", "Value": annotator_id})

                scope_df = pd.DataFrame(scope_rows)
                scope_df.columns = ["Key", "Value"]

                # clickable link for http
                def _mk_link(v):
                    if isinstance(v, str) and v.startswith("http"):
                        return f"[link]({v})"
                    return v

                scope_df["Value"] = scope_df["Value"].apply(_mk_link)

                st.dataframe(
                    scope_df.set_index("Key"),
                    use_container_width=True,
                    height=min(400, 35 * len(scope_df) + 40),
                )

            # --- messages -------------------------------------------------------------
            with msg_col:
                st.markdown("### ğŸ’¬ Messages")
                messages = task.get("messages", [])
                for i, msg in enumerate(messages, start=1):
                    role_raw = msg.get("role", "")
                    role = role_raw.capitalize()
                    badge_class = (
                        "rlhf-badge-user"
                        if role == "User"
                        else "rlhf-badge-assistant" if role == "Assistant" else ""
                    )

                    with st.expander(f"{role} msg #{i}", expanded=(i == 1)):
                        st.markdown(
                            f"<span class='{badge_class}'>{role}</span>",
                            unsafe_allow_html=True,
                        )
                        if role == "User":
                            prompt_text = msg.get("text", "")
                            st.markdown("**Prompt (text):**")
                            st.code(prompt_text)

                            st.markdown("**Other Fields in User Message**")
                            for key, val in msg.items():
                                if key in ["text", "role"]:
                                    continue
                                st.markdown(f"##### â–¸ {key}")
                                if key == "prompt_evaluation" and isinstance(val, list):
                                    rows = [
                                        {
                                            "Question": item.get("question", ""),
                                            "Description": item.get("description", ""),
                                            "Human Answer": item.get(
                                                "human_input_value", ""
                                            ),
                                        }
                                        for item in val
                                    ]
                                    _safe_show_eval_rows(rows)
                                else:
                                    if isinstance(val, (dict, list)):
                                        st.json(val)
                                    else:
                                        st.write(val)

                        elif role == "Assistant":
                            signal = msg.get("signal", {})
                            ideal_response = signal.get("ideal_response")
                            if ideal_response:
                                st.markdown("**ğŸ’¡ Ideal Response:**")
                                st.code(ideal_response)

                            human_evals = signal.get("human_evals", [])
                            for eval_set in human_evals:
                                eval_form = eval_set.get("evaluation_form", [])
                                if eval_form:
                                    st.markdown("**ğŸ§¾ Human Evaluation Table:**")
                                    rows = [
                                        {
                                            "Question": item.get("question", ""),
                                            "Description": item.get("description", ""),
                                            "Human Answer": item.get(
                                                "human_input_value", ""
                                            ),
                                        }
                                        for item in eval_form
                                    ]
                                    _safe_show_eval_rows(rows)


    # =============================================================================
    # TAB 2 â€“ VIEW CSV + FILTERS
    # =============================================================================
    with csv_tab:
        st.subheader("Compiled Task Table")
        st.caption(
            "Filters available for: task_id, scope_category, scope_batchName, scope_batchId."
        )

        if data_csv.empty:
            st.warning("No compiled task rows.")
        else:
            with st.expander("ğŸ” Add Filters", expanded=False):
                filtered_df = filter_dataframe(data_csv, allowed_cols=ALLOWED_FILTER_COLS)

            if "filtered_df" not in locals():
                filtered_df = data_csv.copy()

            st.markdown(
                f"**Showing {len(filtered_df):,} of {len(data_csv):,} rows.**",
                unsafe_allow_html=True,
            )

            st.dataframe(filtered_df, use_container_width=True)

            # Sticky toolbar
            st.markdown("<div class='rlhf-toolbar-sticky'></div>", unsafe_allow_html=True)

            # Create binary Excel files in memory
            full_buf = io.BytesIO()
            filt_buf = io.BytesIO()

            # Save Excel files to buffers
            with pd.ExcelWriter(full_buf, engine='xlsxwriter') as writer:
                data_csv.to_excel(writer, index=False)
            with pd.ExcelWriter(filt_buf, engine='xlsxwriter') as writer:
                filtered_df.to_excel(writer, index=False)

            # Rewind the buffer to the beginning
            full_buf.seek(0)
            filt_buf.seek(0)

            # Download buttons
            col_dl1, col_dl2 = st.columns(2)
            with col_dl1:
                st.download_button(
                    label="â¬‡ï¸ Download FULL Data",
                    data=full_buf,
                    file_name="rlhf_tasks_export_full.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            with col_dl2:
                st.download_button(
                    label="â¬‡ï¸ Download FILTERED Data",
                    data=filt_buf,
                    file_name="rlhf_tasks_export_filtered.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )


    # =============================================================================
    # TAB 3 â€“ DELIVERY BATCH CREATOR
    # =============================================================================
    with delivery_tab:
        st.subheader("Delivery Batch Creator")
        st.caption(
            "Upload an *Ingestion JSON* (work items) below. We'll build a Delivery payload from the compiled tasks."
        )

        ingestion_file = st.file_uploader(
            "Upload Ingestion JSON", type="json", key="ingestion_json_uploader_tab"
        )
        ingestion_data = load_json(ingestion_file)

        if ingestion_file and ingestion_data is None:
            st.stop()

        if ingestion_data is None:
            st.info("Upload an ingestion JSON to enable batch creation.")
        else:
            st.success("Ingestion JSON loaded.")
            
            # Add the two user input controls
            st.markdown("### âš™ï¸ Configuration Settings")
            
            # Create two columns for the inputs
            config_col1, config_col2 = st.columns(2)
            
            with config_col1:
                # 1. Turns associated dropdown (1-10)
                min_turns_required = st.selectbox(
                    "Minimum Turns Required",
                    options=list(range(1, 11)),
                    index=2,  # Default to 3 (index 2 in 1-10 range)
                    help="Groups with fewer than this number of turns will be skipped",
                    key="min_turns_select"
                )
            
            with config_col2:
                # 2. PDF identifier field dropdown
                available_columns = data_csv.columns.tolist()
                
                # Set default to scope_pdf_name if it exists, otherwise first column
                default_pdf_col = "scope_pdf_name" if "scope_pdf_name" in available_columns else available_columns[0]
                default_index = available_columns.index(default_pdf_col) if default_pdf_col in available_columns else 0
                
                pdf_identifier_field = st.selectbox(
                    "PDF Identifier Field",
                    options=available_columns,
                    index=default_index,
                    help="Column used to group turns for delivery batch creation",
                    key="pdf_identifier_select"
                )

            # Show current configuration
            st.info(f"ğŸ“Š Current Config: Min turns = {min_turns_required}, Grouping field = '{pdf_identifier_field}'")

            # Modified function call with user inputs
            final_json_dict = build_delivery_json_from_ingestion(
                ingestion_data=ingestion_data,
                data_csv=data_csv,
                data_sft=data_sft,
                pdf_col=pdf_identifier_field,  # Use user-selected column
                min_turns=min_turns_required   # Pass minimum turns parameter
            )

            if final_json_dict:
                workitems = final_json_dict.get("workitems", [])
                total_wi = len(workitems)
                show_max = 3

                st.markdown(
                    f"<div class='rlhf-delivery-summary'>Total workitems built: "
                    f"<strong>{total_wi}</strong>. Showing first {min(show_max, total_wi)} below.</div>",
                    unsafe_allow_html=True,
                )

                preview_dict = dict(final_json_dict)
                preview_dict["workitems"] = workitems[:show_max]

                expand_full = st.checkbox(
                    "Show full JSON inline (may be large)", value=False, key="show_full_json"
                )
                if expand_full:
                    st.json(final_json_dict)
                else:
                    st.json(preview_dict)

                # download full JSON
                final_json_str = json.dumps(final_json_dict, ensure_ascii=False, indent=2)
                final_json_bytes = final_json_str.encode("utf-8")
                fname = timestamped_json_filename()

                st.download_button(
                    label="ğŸ“¦ Download FULL Final JSON",
                    data=final_json_bytes,
                    file_name=fname,
                    mime="application/json",
                    key="download_final_json_tab",
                )

                # download CSV
                csv_buf = io.StringIO()
                data_csv.to_csv(csv_buf, index=False)
                st.download_button(
                    label="ğŸ§¾ Download Task CSV",
                    data=csv_buf.getvalue(),
                    file_name="rlhf_tasks_export.csv",
                    mime="text/csv",
                    key="download_task_csv_tab",
                )
            else:
                st.warning("No delivery batch could be built (see warnings/errors above).")






# =============================================================================
# JSON VISUALIZER TAB (Standalone)
# =============================================================================
elif option == "JSON Visualizer":
    st.title("ğŸ“Š JSON Visualizer & Explainer (Powered by GPT-4o)")

    st.markdown("### ğŸ“ Upload Your JSON File")
    json_file = st.file_uploader("Upload JSON File", type="json", key="visualizer_json_uploader")

    if json_file:
        load_dotenv()
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        if not OPENAI_API_KEY:
            st.error("âŒ OpenAI API key not found in .env file")
            st.stop()

        client = openai.OpenAI(api_key=OPENAI_API_KEY)

        json_data = None
        try:
            json_data = json.load(json_file)
        except Exception as e:
            st.error(f"âŒ Failed to read JSON: {str(e)}")

        if json_data:
            st.subheader("ğŸ“„ Uploaded JSON")
            st.json(json_data)

            with st.spinner("Analyzing uploaded JSON in context with GPT-4o..."):
                try:
                    sample_summary = json.dumps(json_data, indent=2)
                    if len(sample_summary) > 10000:
                        sample_summary = sample_summary[:10000] + "\n...\n[TRUNCATED FOR ANALYSIS]"

                    prompt = f"""
You are a JSON specialist. Please deeply analyze the *uploaded JSON* (provided below). Your output must follow this format:

### 1ï¸âƒ£ JSON Format & Structure
- Describe the structure: whether it's an object, array, or list of objects
- Mention relationships between nested fields

### 2ï¸âƒ£ Key Details Table
Prepare a table with the following columns:
| Object | Key | Data Type | Required (Y/N) |
|--------|-----|-----------|----------------|
(Fill in with keys from uploaded JSON and inferred data types. Mark keys used across all records as Required.)

### 3ï¸âƒ£ Purpose of Each Field
- List and explain each key (e.g., `task_id`: uniquely identifies the task, `metadata.pdf_name`: name of the source file, etc.)

### 4ï¸âƒ£ Data Quality Assessment
- Mention any observed issues like incorrect types, inconsistencies, overly verbose text, missing fields, etc.

### 5ï¸âƒ£ Cleaned JSON
- Provide a cleaner and best-practice version of the uploaded JSON using only the visible portion provided below.

```json
{sample_summary}
```
"""

                    response = client.chat.completions.create(
                        model="gpt-4o-2024-05-13",
                        messages=[
                            {"role": "system", "content": (
                                "You are a reasoning expert and a JSON analysis specialist. "
                                "Your job is to deeply analyze uploaded JSON data with step-by-step logic, clear formatting, and structured breakdowns."
                            )},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.7,
                        max_tokens=4096,
                        stream=True
                    )

                    full_output = ""
                    for chunk in response:
                        if chunk.choices[0].delta.content:
                            full_output += chunk.choices[0].delta.content

                    st.markdown("### ğŸ§  Uploaded JSON: Full Breakdown and Cleaned Version")
                    st.markdown(full_output)

                except Exception as e:
                    st.error(f"âŒ Failed to analyze JSON due to: {str(e)}")
    else:
        st.info("ğŸ“¥ Upload a JSON file to begin analysis.")

# =============================================================================
# SCHEMA VALIDATOR  (stand-alone page)
# =============================================================================
elif option == "Schema Validator":
    st.title("ğŸ“ Delivery-JSON Schema Validator")

    st.markdown(
        "Upload the final *delivery batch* JSON **and** a corresponding **Draft-7 schema** "
        "to check structural compliance."
    )

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("#### ğŸ“¤ Delivery JSON")
        delivery_file = st.file_uploader("Upload delivery.json", type="json", key="validator_delivery")
    with col2:
        st.markdown("#### ğŸ“¤ Schema JSON")
        schema_file   = st.file_uploader("Upload schema.json",   type="json", key="validator_schema")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Shared helper (moved from old tab)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from jsonschema import Draft7Validator, ValidationError

    def _format_err(err: ValidationError, item_idx: int, wi: str) -> dict:
        path = " â†’ ".join(map(str, err.path))
        if err.validator == "required":
            brief = f"Missing field(s): {', '.join(err.schema['required'])}"
        elif err.validator == "maxItems":
            brief = f"Too many items â€“ max {err.validator_value}"
        elif err.validator == "type":
            brief = f"Invalid type â€“ expected {err.validator_value}"
        elif err.validator == "pattern":
            brief = f"Pattern mismatch â€“ {err.validator_value}"
        elif err.validator == "oneOf":
            brief = f"Unsupported value â€“ must match one schema in oneOf"
        else:
            brief = err.message
        return {
            "workItemId": wi,
            "item_index": item_idx,
            "error_path": path,
            "brief_message": brief,
            "validator": err.validator,
            "raw_message": err.message,
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Validation logic
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if delivery_file and schema_file:
        delivery  = load_json(delivery_file)
        schema    = load_json(schema_file)

        if not delivery or not schema:
            st.error("âŒ Failed to read one of the files.")
            st.stop()

        if "workitems" not in delivery or not isinstance(delivery["workitems"], list):
            st.error("âŒ `'workitems'` key missing or not a list in delivery JSON.")
            st.stop()

        validator  = Draft7Validator(schema)
        errors_out = []

        for idx, wi in enumerate(delivery["workitems"]):
            wid = wi.get("workItemId", f"INDEX_{idx}")
            for err in validator.iter_errors(wi):
                errors_out.append(_format_err(err, idx, wid))

        if errors_out:
            st.error(f"âŒ Found {len(errors_out)} schema error(s).")
            err_df = pd.DataFrame(errors_out)
            st.dataframe(err_df, use_container_width=True)
        else:
            st.success("âœ… Validation Successful â€” all workitems conform to the schema!")

    elif delivery_file or schema_file:
        st.warning("âš ï¸  Please upload **both** files to run the validator.")
    else:
        st.info("ğŸ“¥ Awaiting uploadsâ€¦")

